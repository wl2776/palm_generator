{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "775b688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python\n",
    "#!apt-get update && apt-get install ffmpeg libsm6 libxext6  -y\n",
    "#!pip install mediapipe\n",
    "#!pip install opencv-python transformers accelerate diffusers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc5c18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1740409652_'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "t = str(int(time.time()))+\"_\"\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a50b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, StableDiffusionXLControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from diffusers.utils import load_image\n",
    "from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, AutoencoderKL\n",
    "from diffusers import DDIMScheduler, EulerAncestralDiscreteScheduler\n",
    "from controlnet_aux import MidasDetector\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae494d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c482b907279486fbc731a9de4759175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "controlnet_conditioning_scale = 1.0  \n",
    "prompt = \"A detailed, high-quality photograph, high-quality  realistically, well lit, of a human palm on a pure white background. White background.The palm is facing the viewer, fingers extended, with the pattern of creases and lines on the hand clearly visible. There are no other objects or elements in the image.\"\n",
    "negative_prompt = \"not a palm, drawn, cartoons, animation, drawing, Blurriness, (many folds), low resolution, other body parts, extraneous objects, background elements, jewelry, tattoos, text, watermarks, multiple hands, gloves, nail polish, distracting details.\"\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"xinsir/controlnet-depth-sdxl-1.0\",\n",
    "    torch_dtype=torch.float16, cache_dir='./modele cache/'\n",
    ")\n",
    "\n",
    "# when test with other base model, you need to change the vae also.\n",
    "vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, cache_dir='./modele cache/')\n",
    "\n",
    "eulera_scheduler = EulerAncestralDiscreteScheduler.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", subfolder=\"scheduler\",cache_dir='./modele cache/')\n",
    "\n",
    "\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    \"John6666/realism-by-stable-yogi-sdxlv2-sdxl\",\n",
    "    controlnet=controlnet,\n",
    "    vae=vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=eulera_scheduler,\n",
    "    cache_dir='./modele cache/'\n",
    ")# .to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a35b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740410054.446164    8842 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1740410054.451797    9523 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: AMD Radeon Graphics (radeonsi, renoir, LLVM 19.1.1, DRM 3.57, 6.8.0-53-generic)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1740410054.481990    9502 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1740410054.518898    9513 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827d4ca9cdf54c9caa8cd120abae2896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "# Инициализация MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,        # Режим обработки статичного изображения\n",
    "    max_num_hands=1,               # Максимальное количество рук для обнаружения\n",
    "    min_detection_confidence=0.95   # Установите порог для обнаружения\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    # Получаем случайный файл маски\n",
    "    mask_files = [f for f in os.listdir('./masks/') if f.endswith((\".jpg\", \".png\"))]\n",
    "    random_mask_file = random.choice(mask_files)\n",
    "    controlnet_img = Image.open('./masks/' + random_mask_file).convert(\"L\")  # Преобразуем в черно-белый формат\n",
    "    width, height = controlnet_img.size\n",
    "\n",
    "    # Генерация изображения с pipe\n",
    "    images = pipe(\n",
    "        prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=controlnet_img,\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        num_inference_steps=30,\n",
    "        num_images_per_prompt=10\n",
    "    )\n",
    "\n",
    "    # Конвертируем маску в NumPy массив для применения\n",
    "    controlnet_mask = np.array(controlnet_img)\n",
    "\n",
    "    # Пройдем по всем сгенерированным изображениям\n",
    "    for im in range(len(images.images)):\n",
    "        #images.images[im].save(\"./img/\" + t + \"image_\" +random_mask_file[:-4]+\"_\"+ str(im) + '_' + str(i) + \".jpg\")\n",
    "        ###############################\n",
    "        # Получаем сгенерированное изображение как NumPy массив\n",
    "        img_array = np.array(images.images[im])\n",
    "        # Создаем белый фон, такого же размера как изображение\n",
    "        white_background = np.full_like(img_array, 255)\n",
    "        # Применяем маску: где маска черная (0), делаем белым\n",
    "        masked_image = np.where(controlnet_mask[..., None] <100, white_background, img_array)\n",
    "        # Конвертируем обратно в PIL изображение\n",
    "        masked_pil_image = Image.fromarray(masked_image)\n",
    "        # Сохраняем изображение\n",
    "        #masked_pil_image.save(\"./img/\" + t + \"image_\" +random_mask_file[:-4]+\"_\"+ str(im) + '_' + str(i) + \".jpg\")\n",
    "        #############################\n",
    "\n",
    "        # Преобразование PIL Image в NumPy массив с цветовым пространством RGB\n",
    "        image_np = np.array(masked_pil_image.convert('RGB'))\n",
    "        \n",
    "        # Обработка изображения\n",
    "        results = hands.process(image_np)\n",
    "        \n",
    "        # Проверка и получение оценки уверенности\n",
    "        if results.multi_hand_landmarks:\n",
    "            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                # Получение информации о классификации руки\n",
    "                hand_classification = results.multi_handedness[idx]\n",
    "                hand_label = hand_classification.classification[0].label  # 'Left' или 'Right'\n",
    "                hand_score = hand_classification.classification[0].score  # Оценка уверенности (от 0 до 1)\n",
    "                probability = hand_score * 100  # Конвертируем в проценты\n",
    "        \n",
    "                # Вывод информации\n",
    "                print(f\"вероятность {probability:.2f}%\")\n",
    "\n",
    "                masked_pil_image.save(\"./img/\" + t + \"image_\" +random_mask_file[:-4]+\"_\"+ str(im) + '_' + str(i)+f\"_{probability:.2f}\" + \".jpg\")\n",
    "\n",
    "        else:\n",
    "            #masked_pil_image.save(\"./img_b/\" + t + \"image_\" +random_mask_file[:-4]+\"_\"+ str(im) + '_' + str(i) + \".jpg\")\n",
    "            print(\"Рука не обнаружена на изображении.\")\n",
    "    \n",
    "    # Очищаем кэш после каждой итерации\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd97ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
